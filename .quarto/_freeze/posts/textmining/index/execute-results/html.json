{
  "hash": "fc5201805baca13e7d107eb425748592",
  "result": {
    "markdown": "---\ntitle: Text Mining Learning Guide \ndescription: A learning guide of text mining with the dataset in RStudio \nauthor: Yiyang Shi, Cecelia Kaufmann, Tam Nguyen\ndate: '2023-03-01'\ncategories: \n  - R\n  - Text Mining\n  - Sentiment Analysis\nimage: textmining.png\n---\n\n\n\n\n\n## Background and Resources -- Cecelia\n+ tidy text analysis (why do we want to do this?)\n+ the package (gutenbergr and what is it)\n+ different lexicons\n+ sentiment analysis \n+ reminder about joins\n\n## Resources\n[Textbook on tidy text]<https://www.tidytextmining.com/index.html>\n\n[Sentiment Analysis and Tidy Tuesday]<https://juliasilge.com/blog/animal-crossing/>\n\n## The tidy text format & Document-Term Matrix(DTM)\n\n* According to the \"Text Mining with R\" textbook, the tidy text format is a table with one-token-per-row. This means that:\n  + Each variable is a column\n  + Each observation is a row\n  + Each type of observation unit is a table\nTherefore, a token is a meaningful unit of text, like a word, that we as data scientists are interested in analyzing. \nFor tidy text mining, we may want to do a process called tokenization which splits words into tokens and then allows us to normally analyze by word. \n\n* In chapter 5 of \"Text Mining with R\", DTM is one of the most common structure that text mining work with, where\n  + Each row represents a document (book or article)\n  + Each column represents one term.\n  + Each value (typically) contains the number of appearances of that term in that document.\n\n\nSince DTM objects and and tidy data frames are two incompactible objects, we cannot use tidy tools to analyze a DTM object. Tidytext package provides two functions that convert between these two formats:\n\n+ tidy() turns a DTM to a tidy dataframe.\n+ cast() turns a tidy one term per row dataframe to a matrix.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# DTM\n# Install the package AssociatedPress before you run this code chunk\ndata(\"AssociatedPress\", package = \"topicmodels\")\n\n# tidying a DTM\nap_td <- tidy(AssociatedPress)\nap_td\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 302,031 × 3\n   document term       count\n      <int> <chr>      <dbl>\n 1        1 adding         1\n 2        1 adult          2\n 3        1 ago            1\n 4        1 alcohol        1\n 5        1 allegedly      1\n 6        1 allen          1\n 7        1 apparently     2\n 8        1 appeared       1\n 9        1 arrested       1\n10        1 assault        1\n# … with 302,021 more rows\n```\n:::\n\n```{.r .cell-code}\n# joining tidy dataframe with sentiments dataframe\nap_sentiments <- ap_td %>% \n  inner_join(get_sentiments(\"bing\"), by = c(term = \"word\"))\n\n# Here is an example from the \"Text Mining with R\"\nap_sentiments %>% \n  count(sentiment, term, wt = count) %>% \n  ungroup() %>% \n  filter(n > 200) %>% \n  mutate(m = ifelse(sentiment == \"positive\", n, -n)) %>% \n  mutate(term = reorder(term, m)) %>% \n  ggplot(aes(x = m, y = term, fill = sentiment)) +\n  geom_col() +\n  labs(x = \"Contribution to Sentiment\", y = \"\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# casting a tidy dataframe\nap_td %>% \n  cast_dtm(document, term, count)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<<DocumentTermMatrix (documents: 2246, terms: 10473)>>\nNon-/sparse entries: 302031/23220327\nSparsity           : 99%\nMaximal term length: 18\nWeighting          : term frequency (tf)\n```\n:::\n:::\n\n\n\n## Accessing the Jane Austin Books\n\n::: {.cell}\n\n```{.r .cell-code}\noriginal_books <- austen_books() %>%\n  group_by(book) %>%\n  mutate(linenumber = row_number(),\n         chapter = cumsum(str_detect(text, \n                                     regex(\"^chapter [\\\\divxlc]\",\n                                           ignore_case = TRUE)))) %>%\n  ungroup()\n\noriginal_books\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 73,422 × 4\n   text                    book                linenumber chapter\n   <chr>                   <fct>                    <int>   <int>\n 1 \"SENSE AND SENSIBILITY\" Sense & Sensibility          1       0\n 2 \"\"                      Sense & Sensibility          2       0\n 3 \"by Jane Austen\"        Sense & Sensibility          3       0\n 4 \"\"                      Sense & Sensibility          4       0\n 5 \"(1811)\"                Sense & Sensibility          5       0\n 6 \"\"                      Sense & Sensibility          6       0\n 7 \"\"                      Sense & Sensibility          7       0\n 8 \"\"                      Sense & Sensibility          8       0\n 9 \"\"                      Sense & Sensibility          9       0\n10 \"CHAPTER 1\"             Sense & Sensibility         10       1\n# … with 73,412 more rows\n```\n:::\n:::\n\n\nNow, to work with the tidy dataset we just created, we need to restructure it into a one-token-per-row format which leads us to our unnest_tokens function\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_books <- original_books %>%\n  unnest_tokens(word, text)\n```\n:::\n\n\nThe unnest_tokens uses the tokenizers package to separate each line of text in the original data frame into tokens. (More on different tyoes of tokenizing later)\n\n\nNow that our data is in a one-word-per-row format, we can use tidy tools (like dplyr). \n\n## Removing Words\nWe can use the tidytext dataset stop_words with an anti_join to remove common English words like \"the\", \"of\", and \"to\" which potentially not be fruitful in a sentiment analysis context. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_books <- tidy_books %>%\n  anti_join(stop_words)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining, by = \"word\"\n```\n:::\n:::\n\n\n## Practice\n1. Find the most common words in all the tidy_books books as a whole. Create a visualization via ggplot to show the most common words in Jane Austen books. \n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_books\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 217,609 × 4\n   book                linenumber chapter word       \n   <fct>                    <int>   <int> <chr>      \n 1 Sense & Sensibility          1       0 sense      \n 2 Sense & Sensibility          1       0 sensibility\n 3 Sense & Sensibility          3       0 jane       \n 4 Sense & Sensibility          3       0 austen     \n 5 Sense & Sensibility          5       0 1811       \n 6 Sense & Sensibility         10       1 chapter    \n 7 Sense & Sensibility         10       1 1          \n 8 Sense & Sensibility         13       1 family     \n 9 Sense & Sensibility         13       1 dashwood   \n10 Sense & Sensibility         13       1 settled    \n# … with 217,599 more rows\n```\n:::\n:::\n\n\n\n# The gutenbergr package\n\nAnother package we will be using for our sentiment analysis is the gutenbergr package, which can give us access to public domain works in the Project Gutenberg <https://www.gutenberg.org/> Collection. This is a huge package that gives us access to a large number of books and metadata around the books.\n\n\nLet's look at some of the Bronte sisters' works. \n\n## Practice\n2. To do: Pepare the gutenberg dataset for the Bronte sisters for sentiment analysis (hint: think unnest_tokens and anti_join). From there, how would we find the the most common words in the novels?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbronte <- gutenberg_download(c(1260, 768, 969, 9182, 767))\n\n## What would we insert in the in the parentheses?\nbronte %>%\n  unnest_tokens(???, ???) %>%\n  anti_join(????)\n\n## Now, use previous examples to find the most common words\n```\n\n::: {.cell-output .cell-output-error}\n```\nError: <text>:5:20: unexpected ','\n4: bronte %>%\n5:   unnest_tokens(???,\n                      ^\n```\n:::\n:::\n\n\n## Section Practice\n\nSo, how do we think we can calculate the frequency of each word for the works of Jane Austin and the Bronte sisters? How would we graph this? \n\n\n::: {.cell}\n\n```{.r .cell-code}\nfrequency <- bind_rows(mutate (bronte, author = \n\"Bronte Sister\"),\nmutate(tidy_books, author = \"Jane Austen\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in mutate(bronte, author = \"Bronte Sister\"): object 'bronte' not found\n```\n:::\n\n```{.r .cell-code}\n ## Can you find a way to use a regx here?\n```\n:::\n\n\n\nHow would we plot this (hint: use ggplot)?\n\n::: {.cell}\n\n:::\n\n\n\nWe can also run correlation tests, which allows us to quantify how similar and different these sets of word frequencies are.\n\nLet's run a Pearson's correlation test between the Bronte sisters and Jane Austins' works. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor.test(data = frequency[frequency$author == \"Brontë Sisters\",],\n         ~ proportion + `Jane Austen`)\n```\n:::\n\n\n## Practice\n\nWhat does this information tell you?\n\n>>>> ANSWER:\n\n\nSentiment Analysis with tidy data\n\nSo what is sentiment analysis? Sentiment Analysis allows us to analyze the emotion in text programmatically. one of the more common ways to do this is to consider the text as a combination of its individual words and the sentiment content of the whole text as the sum of the sentiment content of the individual words. \n\nHow are sentiment lexicons created and validated? They are constructed either via crowdsourcing or by an individual which they was validated using crowdsourcing, restaurant or movie reviews, or Twitter data. \n\nThere are a few different lexicon databases that can be used to do sentiment analysis (read more here <>) but for this we will use the nrc lexicon. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_sentiments(\"nrc\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 13,872 × 2\n   word        sentiment\n   <chr>       <chr>    \n 1 abacus      trust    \n 2 abandon     fear     \n 3 abandon     negative \n 4 abandon     sadness  \n 5 abandoned   anger    \n 6 abandoned   fear     \n 7 abandoned   negative \n 8 abandoned   sadness  \n 9 abandonment anger    \n10 abandonment fear     \n# … with 13,862 more rows\n```\n:::\n:::\n\n***NOTE: THIS WILL TAKE A WHILE TO DOWNLOAD***\n\nThe nrc lexicon works by giving a list of English words and then giving their association to eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). The annotations for the lexicon is collected manually through crowd sourcing. \n\nTo explore more about the nrc lexicon: <https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm#:~:text=The%20NRC%20Emotion%20Lexicon%20is,were%20manually%20done%20by%20crowdsourcing.>\n\n\nAs shown below, once we add new variables and organize the book so each word has a distinct row, we want to use an inner_join to find the words in common in the book Emma with the \"joy\" words (or nrc_join dataset) in the nrc lexicon. \n\nLet's try an example: What are the most common joy words in the book Emma?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_books <- austen_books() %>%\n  group_by(book) %>%\n  mutate(\n    linenumber = row_number(),\n    chapter = cumsum(str_detect(text, \n                                regex(\"^chapter [\\\\divxlc]\", \n                                      ignore_case = TRUE)))) %>%\n  ungroup() %>%\n  unnest_tokens(word, text)\n\nnrc_joy <- get_sentiments(\"nrc\") %>% \n  filter(sentiment == \"joy\")\n\ntidy_books %>%\n  filter(book == \"Emma\") %>%\n  inner_join(nrc_joy) %>%\n  count(word, sort = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining, by = \"word\"\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 301 × 2\n   word          n\n   <chr>     <int>\n 1 good        359\n 2 friend      166\n 3 hope        143\n 4 happy       125\n 5 love        117\n 6 deal         92\n 7 found        92\n 8 present      89\n 9 kind         82\n10 happiness    76\n# … with 291 more rows\n```\n:::\n:::\n\n\n\n# Practice: How many positive and negative words are in each of the sections of the book? Here is some starter code to help you out!\n\n\n::: {.cell}\n\n```{.r .cell-code}\njane_austen_posneg <- tidy_books %>%\n  inner_join(get_sentiments(\"nrc\")) %>%\n  count(book, ______, %/% 80,\n       # We are using 80 just because of the text \n        sentiment) %>%\n  # pivot_wider into sentiment and get values from the count (n)\n%>% \n  mutate(sentiment = positive - negative)\n```\n:::\n\n\nNow try and plot the results! Creat the graph however you think best fits the previous results.\n\n\n::: {.cell}\n\n:::\n\n\n\nPractice: What are the most common positive and negative words? Use the nrc database and tidy_books. We will want to use an inner_join and a count(). \n\n\n\n\n::: {.cell}\n\n:::\n\n\n\nNow, make a graph with this information\n\n\n::: {.cell}\n\n:::\n\n\n\nOne cool thing with sentiment analysis is we customize our lists, like for example the word \"miss\" is coded as negative but can also be used as a title for a young, unmarried women in Jane Austin's works. We can use bind_rows() to solve this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfigure_custom_words <- bind_rows(tibble(word = c(\"miss\"), lexicon = c(\"custom\")),\n                                 stop_words)\n```\n:::\n\n\n\n\n\n#### Introduction to the tf-idf statistic - Tam\n\nQuestion: When we look at a body of literature works, say J.K.Rowling's Harry Potter series, and want to know what words/terms are more prominent in one book than in others (therefore can potentially tell us about a character or event specifically tied to that book), how do we do it?\n\n`tf` : term frequency, the number of appearances a word makes over total words in a document (%).\n\nproblem: the most frequently used words in English tend to be `stopwords` like \"the\", \"of\" or \"like\", which generally are not that important except in some cases. Therefore, we need a better metric to reflect the true value of a word or phrase.\n\n\n`idf`: inverse document frequency, which is the natural log of the total number of documents divided by the number of documents containing the term we want to examine. The `idf` is a measure that penalizes commonly used words by decreasing their weights but rewarding less commonly used words by increasing their weights. Its formula is as follows \n\n                \n$$idf(\\textrm{term}) = \\ln \\left( \\frac{n_{\\textrm{documents}}}{n_{\\textrm{documents containing term}}}\\right)$$",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}